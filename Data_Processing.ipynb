{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "Hello World\n",
      "Hello World\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")\n",
    "print(\"Hello World\")\n",
    "print(\"Hello World\")\n",
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ?Format            Metric  Year  Number of Records Value (Actual)\n",
      "0               CD             Units  1973                  1            NaN\n",
      "1               CD             Units  1974                  1            NaN\n",
      "2               CD             Units  1975                  1            NaN\n",
      "3               CD             Units  1976                  1            NaN\n",
      "4               CD             Units  1977                  1            NaN\n",
      "...            ...               ...   ...                ...            ...\n",
      "3003  Vinyl Single  Value (Adjusted)  2015                  1    6.205390253\n",
      "3004  Vinyl Single  Value (Adjusted)  2016                  1    5.198931395\n",
      "3005  Vinyl Single  Value (Adjusted)  2017                  1     6.33967756\n",
      "3006  Vinyl Single  Value (Adjusted)  2018                  1    5.386196747\n",
      "3007  Vinyl Single  Value (Adjusted)  2019                  1    6.795945687\n",
      "\n",
      "[3008 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\princ\\AppData\\Local\\Temp\\ipykernel_10784\\2254308073.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as nunu\n",
    "data_set = pd.read_csv(\"MusicData.csv\")\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CD' 'Units' 1973 1]\n",
      " ['CD' 'Units' 1974 1]\n",
      " ['CD' 'Units' 1975 1]\n",
      " ...\n",
      " ['Vinyl Single' 'Value (Adjusted)' 2017 1]\n",
      " ['Vinyl Single' 'Value (Adjusted)' 2018 1]\n",
      " ['Vinyl Single' 'Value (Adjusted)' 2019 1]]\n"
     ]
    }
   ],
   "source": [
    "independents = data_set[[\"?Format\",\"Metric\",\"Year\",\"Number of Records\"]].values #values is used to convert data into array format \n",
    "print(independents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " ...\n",
      " ['6.33967756']\n",
      " ['5.386196747']\n",
      " ['6.795945687']]\n"
     ]
    }
   ],
   "source": [
    "dependents = data_set[[\"Value (Actual)\"]].values # values is used to convert data into array format\n",
    "print(dependents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CD' 'Units' 1973.0 1]\n",
      " ['CD' 'Units' 1974.0 1]\n",
      " ['CD' 'Units' 1975.0 1]\n",
      " ...\n",
      " ['Vinyl Single' 'Value (Adjusted)' 2017.0 1]\n",
      " ['Vinyl Single' 'Value (Adjusted)' 2018.0 1]\n",
      " ['Vinyl Single' 'Value (Adjusted)' 2019.0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Handle Missing Values in independent variable \n",
    "import sklearn # sklearn is a python library\n",
    "from sklearn.impute import SimpleImputer #impute is used to indentify missing values in data set and fill in missing values by mean, median and mode\n",
    "imputer = SimpleImputer(missing_values=nunu.nan, strategy='mean')  # SimpleImputer is the class of impute\n",
    "imputer = imputer.fit(independents[:,2:3]) # it takes missing values in rows and columns \n",
    "independents[:,2:3 ] = imputer.transform(independents[:,2:3]) #it is used to replace missing values to mean \n",
    "print(independents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ... 23 23 23]\n"
     ]
    }
   ],
   "source": [
    "# Catergorical numerical data:(used to convert data into numeric\\)\n",
    "from sklearn.preprocessing import LabelEncoder #LabelEncoder is a class of sklearn.preprocessing\n",
    "Label_independents = LabelEncoder() \n",
    "independents=Label_independents.fit_transform(independents[:,0]) # we convert our categorical data into numeric data\n",
    "print(independents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Now we convert our NUMERICAL data into Binary format(0,1) using dummy incodings \n",
    "from sklearn.preprocessing import OneHotEncoder #OneHotEncoder is a class of sklearn.preprocessing\n",
    "One_hotencoder = OneHotEncoder()\n",
    "independents=One_hotencoder.fit_transform(data_set.Metric.values.reshape(-1,1)).toarray()  # we convert our data into dummy incodings \n",
    "print(independents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1138 1138 1138 ...  936  861  941]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\princ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "Label_dependents = LabelEncoder()\n",
    "dependents=Label_dependents.fit_transform(dependents)\n",
    "print(dependents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "One_hotencoder = OneHotEncoder()\n",
    "dependents=One_hotencoder.fit_transform(data_set.Metric.values.reshape(-1,1)).toarray()  # we convert our data into dummy incodings \n",
    "print(dependents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data sets________________________________________________________________\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(independents,dependents,test_size=0.2,random_state=0) #0.2 means 20% data used to train and random value cannot generate so we assigin random value is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62252972 -0.74381461  1.32039523]\n",
      " [ 1.60634901 -0.74381461 -0.757349  ]\n",
      " [ 1.60634901 -0.74381461 -0.757349  ]\n",
      " ...\n",
      " [-0.62252972  1.34442103 -0.757349  ]\n",
      " [-0.62252972 -0.74381461  1.32039523]\n",
      " [-0.62252972 -0.74381461  1.32039523]]\n"
     ]
    }
   ],
   "source": [
    "# standard scaler \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train) \n",
    "X_test = sc_x.transform(X_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62252972  1.34442103 -0.757349  ]\n",
      " [ 1.60634901 -0.74381461 -0.757349  ]\n",
      " [-0.62252972  1.34442103 -0.757349  ]\n",
      " ...\n",
      " [-0.62252972  1.34442103 -0.757349  ]\n",
      " [-0.62252972 -0.74381461  1.32039523]\n",
      " [-0.62252972  1.34442103 -0.757349  ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Our data set is ready to feed any models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
